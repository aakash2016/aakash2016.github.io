---
layout: archive
permalink: /reflections/cse255/
author_profile: true
---

<div style="display: flex; align-items: center; font-size: 14px; font-family: 'Times New Roman', Times, serif; color:rgb(0, 0, 0); margin-top: 15px;">
    Sharing my Reflections on the courses I took as a Data Science major (MS) student at UCSD
</div>

<div style="justify-content: center; align-items: center; font-family: 'Times New Roman', Times, serif;">
  <div style="flex: 1; font-size: 14px; color: #212f3c;">
    <h3 style="color: #1e3a8a; font-size: 24px; font-family: 'Times New Roman', Times, serif;">CSE255: Data Mining and Analytics</h3>
    <p><strong style="color: black; font-size: 16px;">Instructor: Prof. Yoav Freund</strong></p>
    <!-- <div style="text-align: center;">
      <figure style="display: inline-block; text-align: center; position: relative;">
        <img src="/assets/images/cse255.png" alt="CSE 257" style="width: 500px; height: auto;">
        <figcaption style="font-size: 12px; color: #555;">Prof Freund teaching us the core of AdaBoost algorithm.</figcaption>
      </figure>
    </div> -->
    <p style="font-size: 14px; color: #212f3c; text-align: justify;">
      This course was highly valuable from a practical standpoint, offering insights into core concepts that underpin real-world data science and analytics in corporate environments. It helped me better understand and appreciate several subtle and seemingly simple ideas in data science that are often overlooked. Prof. Yoav Freund, who is notably one of the authors of the AdaBoost algorithm, has made significant contributions to the field. Learning from him and hearing his insights on several key topics gave me a fresh perspective on ideas that are otherwise considered routine in data science. <br><br>
      The course delved into the following major topics:
      <ul style="font-size: 14px; color: #212f3c; text-align: justify;">
        <li><strong>Throughput vs. latency</strong>: Learned that the primary bottleneck in big data processing is often data movement, not computation. The concepts of latency and throughput lie at the heart of understanding and optimizing big data systems. Also learned in depth about caching, locality of storage accesses and the memory hierarchy.</li>
        <li><strong>Distributed Computing</strong>: Learned the fundamentals of distributed computing through frameworks like Hadoop, including concepts like chunking in HDFS/GFS and their properties. Also studied Hadoop MapReduce and how it compares to in-memory data processing frameworks like Spark.</li>
        <li><strong>Spark</strong>: Gained a deep understanding of Sparkâ€™s architecture and its spatial software organization, including key components such as the Driver, Cluster Manager, Workers, and Executors. </li>
        <li><strong>RDD (Resilient Distributed Datasets)</strong>: Learned about the core Spark data structure, RDDs, which enable distributed and in-memory processing, along with key properties like immutability, fault tolerance, and lineage.</li>
        <li><strong>PCA</strong>: Learned about eigenvectors and PCA in great depth and associated metric like the percent variance explained and the importance of such metrics.</li>
        <li><strong>K-means</strong>: Prof talked about the theoretical properties of k-means algorithm in a great depth. He also talked about different initialisation techniques, how to measure cluster stability, and how sparks <strong>"|| k-means"</strong> parallelises the k-means algorithm.</li>
        <li><strong>Boosting</strong>: Learned about weak learners, decision trees, bagging and boosting algorithms. Prof explained his work on <strong>Adaboost</strong> and the idea of margins. He discussed many theoretical properties of boosting algorithms and talked about some other variants of boostings like brownboost, logitboost, etc.</li>
      </ul>
      Few key-takeways to remember:
      <ul style="font-size: 14px; color: #212f3c; text-align: justify;">
        <li><strong>Latency</strong>: total time to process one unit from start to end. <strong>Throughput</strong>: number of units processed in one minute. Is Throughput = 1 / Latency? <strong>Not necessarily!</strong>. </li>
        <li>Transmitting TB of data through the internet is slow and expensive. Sending a physical disk through <strong>FedEx</strong> is cheap and high bandwidth.</li>
        <li><strong>Heavy Tail Distributions</strong> The probability of outliers is much much higher than the probability given by the normal distribution with the same mean and variance. This phenomenon is common in memory hierarchy. It is a result of using memory caches. The average and std of the latency are dominated by the cache hits, but the <strong>rare cache misses</strong> result in the long tail.</li>
        <li><strong>Achieving locality by being oblivious to order</strong>: To optimize performance, we want both sequential data access (for cache efficiency) and parallel processing (for multi-core scalability). Frameworks like MapReduce or Spark enable this by abstracting away computation order, allowing the system to determine the most efficient execution plan.
        <div style="text-align: center;">
          <figure style="display: inline-block; text-align: center; position: relative;">
            <img src="/assets/images/cs255_mapreduce.png" alt="CSE 257 Map Reduce" style="width: 300px; height: auto;">
          </figure>
        </div>
        </li>
        <li><strong>Busy evaluation</strong> computes and stores all intermediate results, requiring multiple passes over the data. <strong>Lazy evaluation</strong> delays computation until needed, avoiding intermediate storage and enabling a single, efficient pass.</li>
        <li><strong>Repartitioning for Load Balancing</strong>: When partitions are uneven, some workers may be idle while others are overloaded. This imbalance can worsen over time, especially if empty partitions persist. Repartitioning redistributes data evenly across workers to improve parallelism. A common way to do this is using <strong>.partitionBy(k)</strong> to assign a new key-based partitioning.</li>
        <li>Two main ways to manipulate DataFrames: <strong>Imperative manipulation</strong> uses methods like .select and .groupby, where you specify both what result you want and how to compute it. This gives you control over operation order. <strong>Declarative manipulation</strong> (using SQL) focuses only on what result you want, this allows for better optimization, though SQL lacks some advanced analysis primitives like covariance.</li>
        <li><strong>The curse of dimensionality</strong>: Most statistical methods break down at high dimensions K-means breaks down in high dimension. A way out: some high dimensional data has low intrinsic dimension</li>
        <li><strong>Ambient dimension</strong> is the raw dimension given i.e the dimension of the space in which the data is defined. <strong>Intrinsic dimension</strong> measures how much is the variability of the data intrinsically. OR a dimension that captures the variability of the data. If the data has a linear structure: PCA gives the intrinsic dim.</li>
        <li>K-means:</li>
        <ul>
          <li>
            <strong><em>K-means improves RMSE at each step (it either decreases or stays the same)</em></strong>. 
            Step 1 is <strong>Cluster Assignment</strong>, associating each data point with a closer centroid cannot increase RMSE. 
            Step 2 is <strong>Update Centroid</strong>, the point with minimal RMSE for a set of points is the mean of those points.
          </li>
          <li>
            <strong><em>K-means converges after a finite number of iterations</em></strong> because the number of different possible RMSE values is finite, as the number of possible associations of points to centers is finite.
          </li>
          <li>Situations where K-means breaks down: Incorrect k value (#blobs), Wrong initialization, Anisotropically distributed blobs, etc.
          <div style="text-align: center;">
            <figure style="display: inline-block; text-align: center; position: relative;">
              <img src="/assets/images/cs255_kmeans.png" alt="CSE 257 K-Means" style="width: 300px; height: auto;">
            </figure>
          </div>
          </li>
        </ul>
        <li><strong>Boosting decision trees - Test error decreases even after training is reduced to 0. Why?</strong> because of <strong>margins</strong>: model pushes training examples further away from the decision boundary.</li>
      </ul>
      <p style="font-size: 14px; color: #212f3c; text-align: justify;">
        The HW assignments were pretty hands on (analysis of climate and weather pattherns of different states in the US) and were a major component of the course. Overall, while some components overlapped with other ML courses, it was valuable to gain a different perspective.
      </p>
    </p>
    <p style="font-size: 14px; color: #ec407a;">Spring 2025</p>
  </div>
</div>
