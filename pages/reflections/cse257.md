---
layout: archive
permalink: /reflections/cse257/
author_profile: true
---

<div style="display: flex; align-items: center; font-size: 14px; font-family: 'Times New Roman', Times, serif; color:rgb(0, 0, 0); margin-top: 15px;">
    Sharing my Reflections on the courses I took as a Data Science major (MS) student at UCSD
</div>

<div style="justify-content: center; align-items: center; font-family: 'Times New Roman', Times, serif;">
  <div style="flex: 1; font-size: 14px; color: #212f3c;">
    <h3 style="color: #1e3a8a; font-size: 24px; font-family: 'Times New Roman', Times, serif;">CSE257: Search & Optimization</h3>
    <p><strong style="color: black; font-size: 16px;">Instructor: Prof. Sicun Gao </strong></p>
    <div style="text-align: center;">
      <figure style="display: inline-block; text-align: center; position: relative;">
        <img src="/assets/images/cse257.png" alt="CSE 257" style="width: 500px; height: auto;">
        <figcaption style="font-size: 12px; color: #555;">Prof Sean teaching us TD. His classes are quite rigorous and engaging.</figcaption>
      </figure>
    </div>
    <p style="font-size: 14px; color: #212f3c; text-align: justify;">
      This course was a standout experience in my Master’s at UCSD. Prof. Sean Gao’s elegant teaching style made optimization and reinforcement learning truly engaging. What I liked the most about Prof Gao is the time he spends to address each and every doubt of the students. <br><br>
      The course delved into the following major themes:
      <ul style="font-size: 14px; color: #212f3c; text-align: justify;">
        <li><strong>Numerical Optimization</strong>: Learned basics of optimization like the first and second order necessary and sufficient conditions for optimality, algorithms like the Gradient Descent and its variants including the exact line search, Newton direction, etc.</li>
        <li><strong>Stochastic Optimization</strong>: These include techniques involving random walks like the SANN (Simulated Annealing), sample based methods like CEM (Cross Entropy Methods), and Search Gradient (similar to Policy Gradient).</li>
        <li><strong>RL</strong>: Learned about zero-sum game algorithms like minimax and expectimax and how to formulate any given problem as an MDP via core elements like <State space, Action space, Rewards, Transition model, discount factor>. The Prof talked about the Value iteration and the Policy Iteration algorithms. Model-free approaches to learning or estimating values such as: TD (Temporal Difference) policy evaluation, MC (Mnote Carlo) policy evaluation and Q-learning were covered in great detail. </li>
        <li><strong>Regret Analysis and MCTS</strong>: Learned about the analysis of regret for stochastic policies like: ETC (Explore-then-commit), epsilon-greedy and UCB (Upper Confidence Bound), all of which incur-sublinear regrets. Learned to use UCB for online search algorithms like the MCTS.</li>
      </ul>
      The assignments were quite hands-on and gave me the flavor of RL and Search.<br><br>
      Few key-takeways to remember:
      <ul style="font-size: 14px; color: #212f3c; text-align: justify;">
        <li>Policy is not an element of the MDP. Its just an agency that the agent uses to navigate the environment.</li>
        <li><strong>Why do we need discounting?</strong>Without discounting, the sum of future rewards is potentially infinite. How will we know which policy is better if both the policies being compared yield an infinite return. In RL, all we care about is learning a good polcy.</li>
        <li>Bellman update is a <strong>contraction mapping</strong>, which has a unique fixed point (aka singularity).</li>
        <li><strong>Why we learn evaluations and policies?</strong> We dont know the MDP (specifically the transition model of the environment i.e. P(s' | s, a)) in reality. Hence, we use sampling based methods like MC or TD for policy evaluation. Note, TD is online and incremental, whereas in MC (offline) policy evaluation we need to wait for the entire sequence of actions to happen to calculate the return (It is not utilizing the forms of Bellman equations, hence missing opportunities for updating values on the fly). MC, TD are about evaluating the state values based on a fixed policy. </li>
        <li>In reality, no one gives us the policy, we need to figure that out and one of the ways is epsilon-greedy approach.</li>
        <li>Q-learning is off-policy (because of the max). Here, the Q-values are only reliable when we visit each state-action pair a large number of times. Deep-Q learning is just replacing Q-tables with a NN.</li>
        <li>We should always check for action space first in practical problems. In continuous action spaces, Q-learning becomes less useful because max_a Q(s, a) becomes an optimization problem in itself. Hence, we should check if its possible to discretise the action space.</li>
        <li><strong>Why we dont hear much about Q-learning in finetuning-LMs?</strong> Q-learning attemps a global optimization in the sense that we form an optimal policy by knowing about the entire space of state, action pairs (uses max). When we have limited samples as when finetuning-LMs, we want things to be local and this is where policy-gradient chips in. Policy gradient is on-policy.</li>
        <li>Numerical Optimization only provides us a <strong>local view</strong> or notion of optimization. In high dimension there can be exponentially many neighborhoods to explore and techniques like gradient descent wont help. Setting gradient equals 0 will lead to exponentially many solutions. </li>
        <li>Stochastic optimization methods give us a global view of optimization and in most cases are guaranteed to converge (find a global solution).</li>
      </ul>
    </p>
    <p style="font-size: 14px; color: #ec407a;">Winter 2025</p>
  </div>
</div>
