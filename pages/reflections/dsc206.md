---
layout: archive
permalink: /reflections/dsc206/
author_profile: true
---

<div style="display: flex; align-items: center; font-size: 14px; font-family: 'Times New Roman', Times, serif; color:rgb(0, 0, 0); margin-top: 15px;">
    Sharing my Reflections on the courses I took as a Data Science major (MS) student at UCSD
</div>

<div style="justify-content: center; align-items: center; font-family: 'Times New Roman', Times, serif;">
  <div style="flex: 1; font-size: 14px; color: #212f3c;">
    <h3 style="color: #1e3a8a; font-size: 24px; font-family: 'Times New Roman', Times, serif;">DSC206: Algorithms for Data Science</h3>
    <p><strong style="color: black; font-size: 16px;">Instructor: Prof. Arya Mazumdar </strong></p>
    <div style="text-align: center;">
    </div>
    <p style="font-size: 14px; color: #212f3c; text-align: justify;">
      This course covered key data science algorithms across four core areas: Dimensionality Reduction, Clustering, Streaming Algorithms, and Optimization. It emphasized mathematical foundations through derivations and proofs, providing a deeper understanding of these techniques.
      <ul style="font-size: 14px; color: #212f3c; text-align: justify;">
        <li><strong>Dimensionality Reduction</strong>: Learned about SVD, low-rank approxiimations and PCA algorithms. There are two major ways to find the singular vectors: one is the power method and the other is the greedy approach. <strong>HITS algorithm</strong> (used for ranking webpages according to a term) is essentially SVD on the web. Also learned about the <strong>Pagerank algorithm</strong>, based on the markov chain model. </li>
        <li><strong>Hashing</strong>: The goal is to find similar queries in a high-dimensional space. We use hashing to replace large sets/docs by much smaller representations called <strong>signatures</strong>. One of the prominent techniques used to compute signatures is <strong>Minhashing</strong>. Another way to find similar queries is using the the <strong>Locality Sensitive Hashing (LSH)</strong>, where the key idea is to hash the items several times such that similar items are more likely to be hashed in the same bucket than dissimilar items.</li>
        <li><strong>Data Streaming</strong>: Learned about practical algorithms like the <strong>Bloom Filter</strong> (used for spam filtering), <strong>Flajolet Martin algorithm</strong> (used for finding the #distinct_items in a stream) and <strong>Count-min sketch</strong> (used for estimating the frequencies of items in a stream). These methods address constraints related to memory limitations, storage availability, and real-time processing demands. CMS is used in Pandas and some of the famous attention algorithms like Hyper-attention.</li>
        <li><strong>Clustering</strong>: Learned about center-based clustering methods like: k-center, k-median, and k-means clustering. Learned about K-means++, which is a better initialization strategy for the <strong>Lloyds algorithm</strong>. It uses FT (Farthest Travel) algorithm for initialization (can be fooled by outliers). Prof also talked about the <strong>spectral clustering</strong> algorithm in great depth and its applications in finding clusters in social networks and communities of interest.</li>
        <li><strong>Optimization and ML</strong>: Had a revisit of some of the supervised learning algorithms like the Perceptron algorithm, SVM, optimization algorithms like GD, SGD and learned about the analysis of their convergence.</li>
      </ul><br>
      Few key-takeways to remember:
      <ul style="font-size: 14px; color: #212f3c; text-align: justify;">
        <li>PCA is just a special case of SVD where we stop at the first signular vector.</li>
        <li>In HITS algorithm, each webpage is composed of two different kinds of scores: Hubs and Authorities.</li>
        <li>When using minhash signatures, the similarity between two sets (in high dimension) = similarity between their signature vectors.  </li>
        <li><strong>Why we need LSH when we already have minhash?</strong> Even though the signature length per doc is small, the possible combination of pairs is huge and it will take weeks to compute the similarity between signatures.</li>
        <li>Data Streaming algorithms are based on the idea that approximate solutions might be faster than exact solutions.</li>
        <li>K-means is an NP-hard optimization problem in itself. In practice <strong>Lloyd's algorithm</strong> is used, which results in an equivalent surrogate loss function. This algorithm always coverges to a local minima of the cost.</li>
      </ul>
    </p>
    <p style="font-size: 14px; color: #ec407a;">winter 2025</p>
  </div>
</div>
